# Streaming / Windowed Analytics Patterns

Apache Flink
	•	Weighted Average (UDAF) – A Flink Table API user-defined aggregate function that computes a weighted average. It defines explicit accumulate and retract methods for adding and removing values, making it an invertible per-key aggregator. This pattern can be expressed as an invertible fold (summing weights and values) and fits well into a Skip-style enriched reducer model.
	•	Top-2 per Group (Table Aggregate) – A Flink table aggregate function that emits the top two values for each group. It updates an accumulator holding the highest and second-highest values on each append, but has no explicit removal logic in the simple implementation. Maintaining top-k requires storing multiple values (enriched state), so while it’s a per-key aggregation, it’s not a simple invertible fold (removal of the max needs additional info). This is compatible with a Skip reducer if enriched state (like storing two largest values) is allowed.
	•	Sliding Window Sum – Using Flink’s DataStream API with a sliding event-time window (e.g., 1-hour window sliding every 5 minutes) to continuously sum values per key. The system automatically handles expiring old events; user logic only appends new values (e.g., via .timeWindow(...).sum(...)). This is essentially a per-key fold (addition) and if needed could use invertible logic (subtract old values) under the hood. It’s well within Skip’s model as an invertible accumulation.
	•	Session Window Count – A Flink session window grouping that counts events per session (windows close after a period of inactivity). This uses only append updates (each event increases the count in its session window) and the runtime merges windows when events bridge the inactivity gap. The aggregation itself (count) is invertible, but defining sessions inherently relies on event timing and window merging (beyond a basic fold). Still, counting per session could fit Skip’s model if the session identification (enriched state tracking last event time) is handled.
	•	Retraction for Corrected Events – A Flink streaming SQL example where late arriving corrections trigger retractions in aggregates. For instance, an UDAF like first_value will retract (remove) an out-of-date value and accumulate the new update, adjusting the result. This uses explicit add/remove logic and is implemented with retractable accumulators, which is exactly the invertible-reducer pattern Skip supports.
	•	Approximate Distinct Count (HLL) – Using a HyperLogLog-based UDAF or built-in function to estimate distinct counts in real time (e.g., Flink’s use of HLL for unique visitor counts) ￼. The aggregator only receives new elements (each update merges into the HLL state), with no per-element removal. While the HLL sketch merge is invertible only if we could subtract sets (not generally possible), it’s an algebraic aggregate that doesn’t depend on ordering. It’s not a simple reversible fold, but as a stateful sketch it could be integrated into Skip if treated as enriched state.

Kafka Streams
	•	Continuous Count (KTable) – A streaming count of events per key (e.g. classic word count) using groupByKey().count() which maintains an ever-updating KTable. Each new event increments the count, and Kafka Streams automatically outputs updated counts; no explicit remove needed unless a tombstone arrives. This is a simple per-key additive fold (increment count) and is compatible with an invertible reducer model (a decrement could remove an event if needed, though Kafka Streams doesn’t require user-specified inverse).
	•	Sliding Window Average – A windowed aggregation computing the average over a 0.5-second sliding window for each sensor. The implementation uses .windowedBy(...).aggregate() to accumulate sum and count in an object, emitting the average for each window. Only additions occur during each window’s life (old windows close naturally). Because sum and count are kept, the aggregator is effectively invertible (one could subtract an old value if needed). It behaves like a per-key invertible fold, so it aligns with Skip’s well-formed reducer (the windowing here is handled outside the aggregator logic).
	•	Session Window Count – Counting events per session window (e.g., clicks per IP with 5-minute inactivity gap) using .windowedBy(SessionWindows.ofInactivityGapAndGrace(...)).count(). The Streams runtime merges sessions and sums their counts, using only append operations (each event starts or joins a session and increments the count). Counting is invertible, but session windowing is inherently order-dependent (windows close after inactivity). The aggregator itself is simple, but the dynamic window boundaries mean this pattern falls outside a pure fold – it requires the system’s windowing logic (Skip could handle it only if it can manage session state as enriched context).
	•	Running Max with KTable – Maintaining the maximum value per key (e.g., highest temperature reading per sensor) via groupByKey().reduce(maxFunc). Each new record updates the state if it’s larger than the current max. There’s no explicit removal – the maximum only ever increases or stays (in the absence of explicit deletions). As an accumulator, “max” isn’t invertible without extra information (if the current max expires or is retracted, the next max is unknown without storing more history). This pattern is not a simple invertible reducer unless extended (e.g., keeping top-N values). It would need enriched state to fit the Skip model.

Apache Beam
	•	Per-Key Running Average – Using a custom CombineFn in Beam to compute the average signal strength per device in an unbounded stream (keeping a running sum and count) ￼. The combiner’s accumulator adds each input and merges partials, but doesn’t remove data (Beam relies on windowing or global state pruning via watermarks). This combine is an algebraic fold (sum and count); it’s invertible in theory, though Beam doesn’t expose an explicit inverse. It can be expressed as a well-formed reducer in Skip (with enriched state holding sum and count).
	•	Tumbling Window Count – A Beam pipeline that groups events into fixed windows (e.g. 5-minute tumbling windows) and counts them via Combine.perKey(Count) ￼. Each window’s data is aggregated by simple addition. Once the window closes (after the watermark), the count is final. This is a straightforward per-key additive aggregation. Within each window it’s a basic fold; beyond that, windowing is a framework concern. It fits Skip’s model on a per-window basis, but the time-bounded nature is external to the reducer.
	•	Session Window Sum – Beam’s session windows (via Window.into(Sessions.withGapDuration(...))) with a subsequent Combine.perKey(Sum) to sum values in each session. As events arrive, Beam assigns them to session groups and sums up the values; if two sessions merge (an event bridges the gap), the runner will merge their accumulators. Summation is invertible, but like Kafka’s sessions, the grouping is time-dependent. The aggregator itself is fine for Skip (a sum), but the session concept implies dynamic grouping logic outside a single reducer’s scope.
	•	Approximate Unique Count – Beam provides ApproximateUnique.perKey() which uses a sketch to estimate the number of distinct elements per key. The CombineFn (often using HyperLogLog) merges probabilistic summaries of elements. It only supports adding new elements (no exact removal of individual elements). This is not invertible in a precise way (you can’t fully “un-count” a single element in a sketch), but as an algebraic sketch merge it doesn’t depend on input order. In Skip’s context, it could be supported as a black-box enriched state aggregate, though not as a simple reversible fold.
	•	Approximate Quantiles – Using Beam’s ApproximateQuantiles.combinePerKey to compute quantiles (e.g., median or top-percentiles) over streaming data. The combiner keeps a compressed data structure of the value distribution and merges these structures from each chunk of data. The computation is holistic (order and the entire multiset matter) but made incremental via approximation. This goes beyond a pure invertible fold, as removals are non-trivial; it would require Skip to accommodate specialized state that captures a summary of the distribution.

Apache Spark
	•	Streaming Word Count (Continuous) – A Structured Streaming query that maintains a running count of words (grouped by word) and updates the counts for each new batch. Spark’s engine increments the counts and outputs updates in update mode. No user-defined inverse function is needed (the engine tracks state between micro-batches). This is a classic per-key cumulative sum (invertible by decrementing, though Spark doesn’t expose it). It cleanly fits the Skip reducer model.
	•	Event-Time Tumbling Window – A Structured Streaming aggregation grouping events into fixed windows, e.g. counting events per 10-minute window with event-time and watermarks ￼. Spark emits one output per window per key after the watermark passes. Inside each window, it’s a normal combiner (e.g., count or sum). The windowing (and later state cleanup via watermark) is external to the aggregator logic. As such, the aggregation per window is a fold (Skip-compatible), but the time-bounding is a framework feature rather than an intrinsic reducer property.
	•	Sliding Window Aggregation – A Structured Streaming query with overlapping windows (e.g., 10-minute windows sliding every 5 minutes) to compute metrics like rolling averages ￼. Spark will update multiple window buckets for each event (each event contributes to several overlapping window groups). The aggregator within each window (sum, count, etc.) is simple and invertible, but because each event belongs to multiple overlapping groups, the overall pattern isn’t a single fold – it’s multiple parallel folds. Skip could handle each window’s fold separately, but the overlapping assignment is a windowing construct outside the reducer model.
	•	DStream Inverted Window Reduce – The legacy Spark Streaming (DStream) API allowed use of an inverse function with reduceByKeyAndWindow so sliding windows could be updated by subtracting expired data. For example, a running 30-second window count can add new events and subtract those older than 30s instead of recomputing from scratch. This explicitly uses an invertible reducer (e.g., add and subtract counts) to maintain the window incrementally. It’s a prime example of a well-formed reducer with enriched state, fully aligned with Skip’s model.
	•	Approximate Distinct Count – Structured Streaming can use approx_count_distinct() (HyperLogLog) in a streaming aggregation, yielding an approximate number of unique elements ￼. The query continuously merges HLL state as new data arrives. The HLL merge is an associative algebraic operation but not strictly invertible. As with the Beam case, Skip could support it as a specialized accumulator, but it isn’t a basic reversible reduction without custom logic.

Materialize (Streaming SQL)
	•	Current Active Count (Temporal Filter) – A Materialize view that counts records currently “active” in a time interval by comparing event timestamps to the moving system time ￼. For example, a query like SELECT content, COUNT(*) FROM events WHERE mz_logical_timestamp() < delete_ts AND mz_logical_timestamp() >= insert_ts GROUP BY content continuously reports how many events per content are valid at the current time. The engine automatically retracts counts when records pass their delete_ts (i.e., expire). This pattern uses the notion of time-bounded validity; the aggregation itself is just a count (invertible), but it relies on the system to remove expired records. It’s compatible with Skip if the reducer can drop or ignore state for expired events (i.e. if enriched with expiration logic).
	•	Top-K per Group – A materialized top-K query that maintains, say, the top 3 items by some metric (e.g. sales) per key. Implemented in Materialize by a subquery and lateral join to select the top 3 values for each group, the engine incrementally updates the result as new data arrives or as ranks change. Internally, this requires keeping a small ordered state for each key to know when an item falls out of the top 3. Top-K is not a simple additive aggregator – removals happen when an item is bumped out of the top set. This requires enriched state (e.g. a bounded min-heap per key), which Skip could support, but it’s outside pure invertible folding. It falls under a holistic but bounded-state reducer pattern.
	•	Aggregated Materialized View – In Materialize, any standard SQL group-by (e.g. summing sales per product) becomes a continually updated aggregation. For instance, a view defined as SELECT product, SUM(amount) FROM sales GROUP BY product will incrementally update as new sales events flow in. The update model is append-only for inserts (each new event adds to the sum, and deletions would generate retractions that subtract). This is equivalent to a per-key fold (addition) with support for retractions. It aligns well with Skip’s well-formed reducer approach – the system essentially implements the invertible accumulator under the hood.
	•	Windowed Aggregation via SQL – Materialize doesn’t have native windowing syntax, but you can emulate fixed windows by grouping on time buckets (e.g., SELECT date_trunc('hour', ts) AS hour, COUNT(*) FROM clicks GROUP BY hour). This yields an hourly rolling aggregation continuously. Each time-bucket acts as a key for an invertible aggregator (count, sum, etc.). That is essentially a tumbling window achieved through the grouping key. It’s a standard fold per key, well-suited to Skip’s model. The “window” boundaries are part of the key, not an intrinsic streaming operation, so the reducer itself remains a normal per-key accumulation.

Each of these patterns highlights how popular streaming systems handle aggregations. Simpler per-key accumulations (sums, counts, averages) tend to use invertible, running accumulators that fit nicely into a Skip-style reducer with enriched state. More complex patterns (session windows, top-K, quantiles) involve time-based grouping or holistic logic, often requiring additional state or system support beyond a basic reversible fold.